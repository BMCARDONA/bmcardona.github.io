<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://bcardona.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bcardona.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-08-05T03:02:38+00:00</updated><id>https://bcardona.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Reading List (Deep Learning)</title><link href="https://bcardona.com/blog/2023/research-papers-reading-list/" rel="alternate" type="text/html" title="Reading List (Deep Learning)" /><published>2023-07-31T20:01:00+00:00</published><updated>2023-07-31T20:01:00+00:00</updated><id>https://bcardona.com/blog/2023/research-papers-reading-list</id><content type="html" xml:base="https://bcardona.com/blog/2023/research-papers-reading-list/"><![CDATA[<p>My reading list of Deep Learning (DL) research papers, books, and articles. These resources were collected primarily from <a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap">this GitHub roadmap</a>, in addition to Andrew Ng’s <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>.</p>

<p>Here’s the key:</p>
<ul>
  <li>✅ = Read</li>
</ul>

<hr />

<h3 id="reading-list">Reading List</h3>

<ul>
  <li>✅ <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">Imagenet classification with deep convolutional neural networks</a> (2012)
    <ul>
      <li>Finished reading on: 7/31/23</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/1409.1556.pdf">Very deep convolutional networks for large-scale image recognition</a> (2014)</li>
  <li><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">Going deeper with convolutions</a> (2015)</li>
  <li><a href="https://arxiv.org/pdf/1512.03385.pdf">Deep residual learning for image recognition</a> (2016)</li>
</ul>]]></content><author><name></name></author><category term="Reading" /><category term="Deep Learning" /><summary type="html"><![CDATA[My reading list of Deep Learning (DL) research papers, books, and articles. These resources were collected primarily from this GitHub roadmap, in addition to Andrew Ng’s Deep Learning Specialization.]]></summary></entry><entry><title type="html">Deep Learning Specialization Solutions</title><link href="https://bcardona.com/blog/2023/deep-learning-specialization/" rel="alternate" type="text/html" title="Deep Learning Specialization Solutions" /><published>2023-07-26T16:42:16+00:00</published><updated>2023-07-26T16:42:16+00:00</updated><id>https://bcardona.com/blog/2023/deep-learning-specialization</id><content type="html" xml:base="https://bcardona.com/blog/2023/deep-learning-specialization/"><![CDATA[<p>As I am nearly finished with Andrew Ng’s <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning specialization</a>, I thought I would create a comprehensive post listing my completed assignments. This course has taught me a very great deal about deep learning and its capabilities, and I encourage anyone interested in the subject to take Ng’s course. Enjoy! :)</p>

<h2 id="programming-assignments">Programming Assignments</h2>

<h5 id="course-1-neural-networks-and-deep-learning">Course 1: Neural Networks and Deep Learning</h5>

<ul>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/1%20-%20Neural%20Networks%20and%20Deep%20Learning/week_2/logistic_regression_with_a_neural_network_mindset">W2A1 - Logistic Regression with a Neural Network mindset</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/1%20-%20Neural%20Networks%20and%20Deep%20Learning/week_2/python_basics_with_numpy">W2A2 - Python Basics with Numpy</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/1%20-%20Neural%20Networks%20and%20Deep%20Learning/week_3/planar_data_classification_with_one_hidden_layer">W3A1 - Planar data classification with one hidden layer</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/1%20-%20Neural%20Networks%20and%20Deep%20Learning/week_4/building_your_deep_neural_network">W4A1 - Building your Deep Neural Network: Step by Step</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/1%20-%20Neural%20Networks%20and%20Deep%20Learning/week_4/deep_neural_network_application">W4A2 - Deep Neural Network for Image Classification: Application</a></li>
</ul>

<h5 id="course-2-improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization">Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</h5>

<ul>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/2%20-%20Improving%20Deep%20Neural%20Networks%3A%20Hyperparameter%20Tuning%2C%20Regularization%2C%20and%20Optimization/week_1/1_initialization">W1A1 - Initialization</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/2%20-%20Improving%20Deep%20Neural%20Networks%3A%20Hyperparameter%20Tuning%2C%20Regularization%2C%20and%20Optimization/week_1/2_regularization">W1A2 - Regularization</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/2%20-%20Improving%20Deep%20Neural%20Networks%3A%20Hyperparameter%20Tuning%2C%20Regularization%2C%20and%20Optimization/week_1/3_gradient_checking">W1A3 - Gradient Checking</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/2%20-%20Improving%20Deep%20Neural%20Networks%3A%20Hyperparameter%20Tuning%2C%20Regularization%2C%20and%20Optimization/week_2/optimization_methods/graded_functions">W2A1 - Optimization Methods</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/2%20-%20Improving%20Deep%20Neural%20Networks%3A%20Hyperparameter%20Tuning%2C%20Regularization%2C%20and%20Optimization/week_3/tensorflow_introduction">W3A1 - Introduction to TensorFlow</a></li>
</ul>

<h5 id="course-3-structuring-machine-learning-projects">Course 3: Structuring Machine Learning Projects</h5>

<p>There are no programming assignments in the third course.</p>

<h5 id="course-4-convolutional-neural-networks">Course 4: Convolutional Neural Networks</h5>

<ul>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/4%20-%20Convolutional%20Neural%20Networks/week_1/1_convolutional_model_step_by_step">W1A1 - Convolutional Model: step by step</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/4%20-%20Convolutional%20Neural%20Networks/week_1/2_convolutional_model_application">W1A2 - Convolutional Model: application</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/4%20-%20Convolutional%20Neural%20Networks/week_2/residual_neural_networks">W2A1 - Residual Networks</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/4%20-%20Convolutional%20Neural%20Networks/week_2/transfer_learning_with_Mobile_Net">W2A2 - Transfer Learning with MobileNet</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/4%20-%20Convolutional%20Neural%20Networks/week_3/1_car_detection_with_yolo">W3A1 - Autonomous Driving - Car Detection</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/4%20-%20Convolutional%20Neural%20Networks/week_3/2_image_segmentation_with_u_net">W3A2 - Image Segmentation - U-net</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/4%20-%20Convolutional%20Neural%20Networks/week_4/1_face_recognition">W4A1 - Face Recognition</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/4%20-%20Convolutional%20Neural%20Networks/week_4/2_art_generation_with_neural_style_transfer">W4A2 - Neural Style transfer</a></li>
</ul>

<h5 id="course-5-sequence-models">Course 5: Sequence Models</h5>

<ul>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/5%20-%20Sequence%20Models/week_1/1_building_your_recurrent_neural_network_step_by_step">W1A1 - Building a Recurrent Neural Network - Step by Step</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/5%20-%20Sequence%20Models/week_1/2_dinosaur_island_character_level_language_modeling">W1A2 - Character level language model - Dinosaurus land</a></li>
  <li><a href="https://github.com/BMCARDONA/Deep-Learning-Specialization-Solutions/tree/main/5%20-%20Sequence%20Models/week_1/3_jazz_improvisation_with_LSTM">W1A3 - Improvise A Jazz Solo with an LSTM Network</a></li>
</ul>

<h3 id="disclaimer">Disclaimer</h3>

<p>If you are currently completing this specialization, I would strongly encourage you <em>not</em> to copy any of these solutions. This specialization is a treasure trove of information, and any hard work you put into it will doubtless be rewarded – please don’t take it for granted!</p>]]></content><author><name></name></author><category term="Deep Learning" /><summary type="html"><![CDATA[As I am nearly finished with Andrew Ng’s Deep Learning specialization, I thought I would create a comprehensive post listing my completed assignments. This course has taught me a very great deal about deep learning and its capabilities, and I encourage anyone interested in the subject to take Ng’s course. Enjoy! :)]]></summary></entry><entry><title type="html">Linear Algebra in a Neural Network Layer</title><link href="https://bcardona.com/blog/2023/vectors-and-matrices/" rel="alternate" type="text/html" title="Linear Algebra in a Neural Network Layer" /><published>2023-06-09T20:42:00+00:00</published><updated>2023-06-09T20:42:00+00:00</updated><id>https://bcardona.com/blog/2023/vectors-and-matrices</id><content type="html" xml:base="https://bcardona.com/blog/2023/vectors-and-matrices/"><![CDATA[<p>It can be difficult to wrap one’s mind around what is happening computationally inside the layer of a neural network between an input matrix, a weight matrix, and a bias vector. Let’s go through an example to see exactly how to calculate the dot product between an input matrix \(\mathbf{\hat{X}}\) and a weight matrix \(\mathbf{\hat{W}}\), how broadcasting is used to add the bias vector \(\mathbf{\hat{b}}\) to the resulting dot product, and why the dimensions of the resulting matrices and vectors make sense.</p>

<p>Suppose we have an \(2 \times 2\) input matrix \(\mathbf{\hat{X}}\), which represents a batch of 2 training examples, with 2 features each:</p>

\[\mathbf{\hat{X}} = \begin{bmatrix}
1 &amp; 2 \\\\
3 &amp; 4
\end{bmatrix}.\]

<p>Suppose that \(\mathbf{\hat{X}}\) serves as the input to a layer with 3 units (neurons). We will therefore have a \(2 \times 3\) weight matrix \(\mathbf{\hat{W}}\), where 2 is given by the number of features of \(\mathbf{\hat{X}}\), and 3 is given by the number of units in the current layer:</p>

\[\mathbf{\hat{W}} = \begin{bmatrix}
0.1 &amp; 0.2 &amp; 0.3 \\\\
0.4 &amp; 0.5 &amp; 0.6
\end{bmatrix}.\]

<p>By taking the dot product of \(\mathbf{\hat{X}}\) and \(\mathbf{\hat{W}}\), we are left with a \(2 \times 3\) matrix:</p>

\[\begin{aligned}
\mathbf{\hat{X}} \cdot \mathbf{\hat{W}} &amp;= \begin{bmatrix}
1 \cdot 0.1 + 2 \cdot 0.4 &amp; 1 \cdot 0.2 + 2 \cdot 0.5 &amp; 1 \cdot 0.3 + 2 \cdot 0.6 \\\\
3 \cdot 0.1 + 4 \cdot 0.4 &amp; 3 \cdot 0.2 + 4 \cdot 0.5 &amp; 3 \cdot 0.3 + 4 \cdot 0.6
\end{bmatrix} \\
&amp;= \begin{bmatrix}
0.9 &amp; 1.2 &amp; 1.5 \\\\
1.7 &amp; 2.4 &amp; 3.1
\end{bmatrix}.
\end{aligned}\]

<p>Let’s assume we have a \(1 \times 3\) bias vector \(\mathbf{\hat{b}}\) (where 3 is given by the number of units in the current layer), representing the bias for each unit in the layer:</p>

\[\mathbf{\hat{b}} = \begin{bmatrix}
0.7 &amp; -0.8 &amp; 0.9
\end{bmatrix}.\]

<p>We will use broadcasting to expand \(\mathbf{\hat{b}}\). (Broadcasting applies to element-wise operations.
Its basic operation is to ‘stretch’ a smaller dimension by replicating elements to match a larger dimension. In this case, we will ‘stretch’ \(\mathbf{\hat{b}}\) by replicating its rows, so that it has a size of \(2 \times 3\), the same dimensions as \(\mathbf{\hat{X}} \cdot \mathbf{\hat{W}}\).) Let’s call this broadcasted matrix \(\mathbf{\hat{B}}\). We have</p>

\[\mathbf{\hat{B}} = \begin{bmatrix}
0.7 &amp; -0.8 &amp; 0.9 \\\\
0.7 &amp; -0.8 &amp; 0.9
\end{bmatrix}.\]

<p>We can now add \(\mathbf{\hat{B}}\) element-wise to \(\mathbf{\hat{X}} \cdot \mathbf{\hat{W}}\):</p>

\[\mathbf{\hat{X}}\mathbf{\hat{W}} + \mathbf{\hat{B}} = \begin{bmatrix}
0.9 + 0.7 &amp; 1.2 - 0.8 &amp; 1.5 + 0.9 \\\\
1.7 + 0.7 &amp; 2.4 - 0.8 &amp; 3.1 + 0.9
\end{bmatrix} = \begin{bmatrix}
1.6 &amp; 0.4 &amp; 2.4 \\\\
2.4 &amp; 1.6 &amp; 4
\end{bmatrix}.\]

<!-- Hence, broadcasting just involves adding the bias vector $$\mathbf{\hat{b}}$$ element-wise to each row of the resulting matrix from the dot product of $$\mathbf{\hat{X}}$$ and $$\mathbf{\hat{W}}$$. -->

<p>It is worth pointing out that the resulting matrix has a size of \(2 \times 3\) (which is the same as \(\mathbf{\hat{X}} \cdot \mathbf{\hat{W}}\)), where \(2\) is given by the number of features of \(\mathbf{\hat{X}}\), and \(3\) is given by the number of units in the current layer.</p>

<p>For completeness, here is a vectorized implemention of the example in Python:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>                  <span class="c1"># dense layer
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>          <span class="c1"># np.matmul() returns the matrix product of two matrices
</span>    <span class="k">return</span> <span class="n">z</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>                <span class="c1"># batch
</span>              <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>       <span class="c1"># weights
</span>              <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>     <span class="c1"># biases
</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="k">return</span> <span class="n">result</span>
</code></pre></div></div>

<p>We may generalize this example by saying that the dimensions of the parameters in a given layer are determined as follows:</p>
<ul>
  <li>If network has \(s_{in}\) units in a layer (i.e., if each training example of \(\mathbf{\hat{X}}\) has \(s_{in}\) features) and \(s_{out}\) units in the next layer (i.e., if the current layer has \(s_{out}\) units/neurons), then
    <ul>
      <li>\(\mathbf{\hat{W}}\) will be of dimension \(s_{in} \times s_{out}\).</li>
      <li>\(\mathbf{\hat{b}}\) will be a vector with \(s_{out}\) elements</li>
    </ul>
  </li>
  <li>Note: A layer’s input dimensions (\(s_{in}\)) are calculated for you. You specify a layer’s output dimensions and this determines the next layer’s input dimension. The input dimension of the first layer is derived from the size of the input data specified in the model.fit statement</li>
</ul>

<p><strong>As an aside</strong>:
It need not be said that I initially had trouble keeping track of the dimensions of the matrices and vectors found in a layer of even a basic neural network. My hope is that this post will provide some insight into how to perform a basic linear algebra computation inside a neural network layer, and why the dimensions of the various vectors and matrices involved make sense. I am currently completing Andrew Ng’s Advanced Learning Algorithms class on Coursera, and will likely use this post as a reference in the months to come.</p>]]></content><author><name></name></author><category term="Machine Learning" /><category term="Deep Learning" /><summary type="html"><![CDATA[It can be difficult to wrap one’s mind around what is happening computationally inside the layer of a neural network between an input matrix, a weight matrix, and a bias vector. Let’s go through an example to see exactly how to calculate the dot product between an input matrix \(\mathbf{\hat{X}}\) and a weight matrix \(\mathbf{\hat{W}}\), how broadcasting is used to add the bias vector \(\mathbf{\hat{b}}\) to the resulting dot product, and why the dimensions of the resulting matrices and vectors make sense.]]></summary></entry><entry><title type="html">Bertrand Russell and Numbers</title><link href="https://bcardona.com/blog/2023/russell/" rel="alternate" type="text/html" title="Bertrand Russell and Numbers" /><published>2023-05-08T23:16:00+00:00</published><updated>2023-05-08T23:16:00+00:00</updated><id>https://bcardona.com/blog/2023/russell</id><content type="html" xml:base="https://bcardona.com/blog/2023/russell/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="Mathematics" /><summary type="html"><![CDATA[Redirecting to another page.]]></summary></entry><entry><title type="html">A Brief Note on Linear Regression</title><link href="https://bcardona.com/blog/2023/linear-regression/" rel="alternate" type="text/html" title="A Brief Note on Linear Regression" /><published>2023-04-11T15:12:00+00:00</published><updated>2023-04-11T15:12:00+00:00</updated><id>https://bcardona.com/blog/2023/linear-regression</id><content type="html" xml:base="https://bcardona.com/blog/2023/linear-regression/"><![CDATA[<p>Linear Regression can be used in supervised machine learning to find a mathematical function that describes the relationship between the input features and the output variables in a training set.</p>

<p>To create such a function, we need a training set composed of (say) two columns: one for the input features and one for the output variables. (We actually would have several columns if we had several input features; for simplicity, though, we will assume in this article that we are dealing with just one input feature.)</p>

<p>Let’s suppose that in our training set our input feature is “house size” and our output variable is “house price.” (Hence, each row or “training instance” of our training set corresponds to a single house). Our goal then is to find a mathematical function that describes the relationship between house sizes and house prices.</p>

<p>If we were to plot each training instance on a graph in the form (house size, house price), then there should exist a best-fit line which minimizes the distance between itself and each point. Such a line can be derived from a “cost function”, which provides a cumulative measure of how well each “prediction” matches its respective “target.” (A prediction is the value that results from our function when we “feed it” an input feature. Its respective target is the output variable in our training set associated with that given input feature.) This measure is called the “cost”. The lower the cost, the better the fit of our line.</p>

<p>We can use linear regression to find the best-fit line; this involves finding the lowest cost associated with the following cost function J:</p>

\[\begin{align*}
J(w, b) = \frac{1}{2m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^{2},
\end{align*}\]

<p>where</p>
<ul>
  <li>\(w\) and \(b\) are “weights” (coefficients whose values can be freely manipulated);</li>
  <li>\(m\) is the “number of training instances”;</li>
  <li>\(i\) is the “\(i^{th}\) training instance”;</li>
  <li>\(x^{(i)}\) is the “\(i^{th}\) input feature”;</li>
  <li>\(y^{(i)}\) is the “\(i^{th}\) output variable” (sometimes called the “\(i^{th}\) target”);</li>
  <li>and \(\hat{y}^{(i)}\) (sometimes written as \(f_{w, b}(x^{(i)})\) or \(w\cdot x^{(i)} + b\)) is the “\(i^{th}\) prediction”.</li>
</ul>

<p>Our goal is to minimize \(J(w, b)\), which requires that we find the “optimal” values of \(w\) and \(b\).</p>

<p>Note: The difference \(\hat{y}^{(i)} - y^{(i)}\) is sometimes referred to as the “loss”. The fact that the cost function squares the loss ensures that the “error surface” is convex like a bowl. Hence, the error surface will always have a minimum that can be reached by following the gradient in all dimensions. By finding the minimum of the error surface, we will have found the values of \(w\) and \(b\) that minimize \(J(w, b)\), thus giving us the best-fit line.</p>]]></content><author><name></name></author><category term="Machine Learning" /><summary type="html"><![CDATA[Linear Regression can be used in supervised machine learning to find a mathematical function that describes the relationship between the input features and the output variables in a training set.]]></summary></entry><entry><title type="html">Reading List (Books)</title><link href="https://bcardona.com/blog/2023/reading-list/" rel="alternate" type="text/html" title="Reading List (Books)" /><published>2023-04-07T12:44:16+00:00</published><updated>2023-04-07T12:44:16+00:00</updated><id>https://bcardona.com/blog/2023/reading-list</id><content type="html" xml:base="https://bcardona.com/blog/2023/reading-list/"><![CDATA[<h4 id="want-to-read">Want to Read:</h4>
<ul>
  <li><em><a href="https://www.amazon.com/Rebooting-AI-Building-Artificial-Intelligence/dp/052556604X/ref=sr_1_1?crid=3IBVCF0P01SV0&amp;keywords=Rebooting+AI&amp;qid=1681313177&amp;sprefix=rebooting+ai%2Caps%2C99&amp;sr=8-1">Rebooting AI</a></em> by Gary Marcus and Ernest Davis.</li>
  <li><em><a href="https://www.amazon.com/Audible-Viral-Search-Origin-COVID-19/dp/B097CLV3QP/ref=sr_1_1?keywords=viral+the+search+for+the+origin+of+covid-19&amp;qid=1681313204&amp;sprefix=Viral%3A+The+Se%2Caps%2C249&amp;sr=8-1">Viral: The Search for the Origin of COVID-19</a></em> by Matt Ridley and Alina Chan.</li>
  <li><em><a href="https://www.amazon.com/Love-World-Journey-Through-Bardos/dp/0525512535">In Love with the World: A Monk’s Journey Through the Bardos of Living and Dying</a></em> by Yongey Mingyur Rinpoche and Helen Tworkov.</li>
</ul>

<hr />

<h4 id="reading">Reading:</h4>
<ul>
  <li><a href="https://www.amazon.com/Build-Career-Science-Jacqueline-Nolis/dp/1617296244/ref=sr_1_13_sspa?crid=38XVJNCF79VTC&amp;keywords=AI+book&amp;qid=1691203969&amp;sprefix=ai+book%2Caps%2C114&amp;sr=8-13-spons&amp;sp_csd=d2lkZ2V0TmFtZT1zcF9tdGY&amp;psc=1">Build a Career in Data Science</a> by Emily Robinson and Jacqueline Nolis.
<!-- - *[Deep Learning with Python](https://www.amazon.com/Learning-Python-Second-Fran-C3-A7ois-Chollet-dp-1617296864/dp/1617296864/ref=dp_ob_title_bk)* by Francois Chollet. --></li>
</ul>

<hr />

<h4 id="have-read">Have Read:</h4>
<ul>
  <li><em><a href="https://www.amazon.com/Alexander-Hamilton-Ron-Chernow/dp/0143034758">Alexander Hamilton</a></em> by Ron Chernow.</li>
  <li><em><a href="https://www.amazon.com/American-Prometheus-Triumph-Tragedy-Oppenheimer/dp/0375726268">American Prometheus</a></em> by Kai Bird and Martin J. Sherwin.</li>
  <li><em><a href="https://www.amazon.com/Aquariums-Pyongyang-Years-North-Korean/dp/0465011047">The Aquariums of Pyongyang</a></em> by Chol-hwan Kang and Pierre Rigoulot.</li>
  <li><em><a href="https://www.amazon.com/Benjamin-Franklin-American-Walter-Isaacson/dp/074325807X/ref=sr_1_1?crid=3LAT4J9JB7C3Q&amp;keywords=Benjamin+Franklin+an+american+life&amp;qid=1680920166&amp;s=books&amp;sprefix=benjamin+franklin+an+american+lif%2Cstripbooks%2C163&amp;sr=1-1">Benjamin Franklin: An American Life</a></em> by Walter Isaacson.</li>
  <li><em><a href="https://www.amazon.com/Brave-New-World-Aldous-Huxley/dp/0060850523">Brave New World</a></em> by Aldous Huxley.</li>
  <li><em><a href="https://www.amazon.com/Deep-Work-Focused-Success-Distracted/dp/1455586692">Deep Work</a></em> by Cal Newport.</li>
  <li><em><a href="https://www.amazon.com/Doing-Good-Better-Effective-Altruism/dp/1592409660">Doing Good Better</a></em> by William MacAskill.</li>
  <li><em><a href="https://www.amazon.com/Foundation-Isaac-Asimov/dp/0553293354">Foundation</a></em> by Isaac Asimov.</li>
  <li><em><a href="https://www.amazon.com/Four-Thousand-Weeks-Management-Mortals/dp/0374159122">Four Thousand Weeks: Time Management for Mortals</a></em> by Oliver Burkeman.</li>
  <li><em><a href="https://www.amazon.com/Free-Will-Deckle-Edge-Harris/dp/1451683405/ref=pd_lpo_1?pd_rd_w=ipq4i&amp;content-id=amzn1.sym.116f529c-aa4d-4763-b2b6-4d614ec7dc00&amp;pf_rd_p=116f529c-aa4d-4763-b2b6-4d614ec7dc00&amp;pf_rd_r=1SM2W5FQGEDX4QPT9PA6&amp;pd_rd_wg=uXanv&amp;pd_rd_r=1b6e771a-623a-4744-9ba8-cbb49874d9bc&amp;pd_rd_i=1451683405&amp;psc=1">Free Will</a></em> by Sam Harris.</li>
  <li><em><a href="https://www.amazon.com/Hitchhikers-Guide-Galaxy-Douglas-Adams/dp/0345391802">The Hitchhiker’s Guide to the Galaxy</a></em> by Douglas Adams.</li>
  <li><em><a href="https://www.amazon.com/History-Western-Philosophy-Bertrand-Russell/dp/0671201581">A History of Western Philosophy</a></em> by Bertrand Russell.</li>
  <li><em><a href="https://www.amazon.com/Hyperion-Cantos-Dan-Simmons/dp/0553283685">Hyperion</a></em> by Dan Simmons.</li>
  <li><em><a href="https://www.amazon.com/Lying-Sam-Harris/dp/1940051002">Lying</a></em> by Sam Harris.</li>
  <li><em><a href="https://www.amazon.com/Making-Sense-Conversations-Consciousness-Morality/dp/B081ZHC3HX/ref=sr_1_1?crid=XA5M07707J09&amp;keywords=sam+harris+conversations&amp;qid=1680921108&amp;s=books&amp;sprefix=sam+harris+conversatio%2Cstripbooks%2C114&amp;sr=1-1">Making Sense: Conversations on Consciousness, Morality, and the Future of Humanity</a></em> by Sam Harris.</li>
  <li><em><a href="https://www.amazon.com/Moral-Landscape-Science-Determine-Values/dp/143917122X">The Moral Landscape</a></em> by Sam Harris.</li>
  <li><em><a href="https://www.amazon.com/Paper-Menagerie-Other-Stories/dp/148142436X/ref=sr_1_1?crid=2BXN2BQAUTYU&amp;keywords=The+Paper+Menagerie&amp;qid=1680920415&amp;s=books&amp;sprefix=the+paper+menagerie%2Cstripbooks%2C80&amp;sr=1-1">The Paper Menagerie</a></em> by Ken Liu.</li>
  <li><em><a href="https://www.amazon.com/Primates-Memoir-Neuroscientists-Unconventional-Baboons/dp/0743202414">A Primate’s Memoir</a></em> by Robert Sapolsky.</li>
  <li><em><a href="https://www.amazon.com/Three-Body-Problem-Boxed-Set-Remembrance/dp/1250254493/ref=sr_1_1?crid=ULZ8K8FJQ4KI&amp;keywords=three+body+problem+trilogy&amp;qid=1680920393&amp;s=books&amp;sprefix=three+body+problem+trilogy%2Cstripbooks%2C114&amp;sr=1-1">The Three Body Problem trilogy</a></em> by Cixin Liu.</li>
  <li><em><a href="https://www.amazon.com/Titan-Life-John-Rockefeller-Sr/dp/1400077303">Titan: The Life of John D. Rockefeller, Sr.</a></em> by Ron Chernow.</li>
  <li><em><a href="https://www.amazon.com/Waking-Up-Sam-Harris-audiobook/dp/B00M9KEFY6/ref=sr_1_1?crid=3RT3TWRQLHK66&amp;keywords=waking+up&amp;qid=1680919942&amp;sprefix=waking+u%2Caps%2C118&amp;sr=8-1">Waking Up: A Guide to Spirituality Without Religion</a></em> by Sam Harris.</li>
  <li><em><a href="https://www.amazon.com/10-Happier-Self-Help-Actually-Works/dp/0062265431">10% Happier</a></em> by Dan Harris.</li>
</ul>]]></content><author><name></name></author><category term="Reading" /><category term="Books" /><summary type="html"><![CDATA[Want to Read: Rebooting AI by Gary Marcus and Ernest Davis. Viral: The Search for the Origin of COVID-19 by Matt Ridley and Alina Chan. In Love with the World: A Monk’s Journey Through the Bardos of Living and Dying by Yongey Mingyur Rinpoche and Helen Tworkov.]]></summary></entry><entry><title type="html">An Infrastructure Cheatsheet</title><link href="https://bcardona.com/blog/2023/infrastructure-cheatsheet/" rel="alternate" type="text/html" title="An Infrastructure Cheatsheet" /><published>2023-04-04T12:44:16+00:00</published><updated>2023-04-04T12:44:16+00:00</updated><id>https://bcardona.com/blog/2023/infrastructure-cheatsheet</id><content type="html" xml:base="https://bcardona.com/blog/2023/infrastructure-cheatsheet/"><![CDATA[<p>This post was inspired by <a href="https://writings.stephenwolfram.com/2019/02/seeking-the-productive-life-some-details-of-my-personal-infrastructure/">Stephen Wolfram’s 2019 post</a>.</p>

<blockquote>
  <p>“Productivity is a trap. Becoming more efficient just makes you more rushed, and trying to clear the decks simply makes them fill up again faster. Nobody in the history of humanity has ever achieved “work-life balance,” whatever that might be, and you certainly won’t get there by copying the “six things successful people do before 7:00 a.m.” The day will never arrive when you finally have everything under control—when the flood of emails has been contained; when your to-do lists have stopped getting longer; when you’re meeting all your obligations at work and in your home life; when nobody’s angry with you for missing a deadline or dropping the ball; and when the fully optimized person you’ve become can turn, at long last, to the things life is really supposed to be about. Let’s start by admitting defeat: none of this is ever going to happen. But you know what? That’s excellent news.” —Oliver Burkeman, <em><a href="https://www.amazon.com/Four-Thousand-Weeks-Management-Mortals/dp/0374159122">Four Thousand Weeks: Time Management for Mortals</a></em></p>
</blockquote>

<h2 id="blogs">Blogs</h2>
<ul>
  <li>DeepLearning.AI’s <a href="https://www.deeplearning.ai/the-batch/">The Batch</a>.</li>
  <li><a href="https://writings.stephenwolfram.com/">Stephen Wolfram’s Writings</a>.</li>
</ul>

<h2 id="general-advice">General Advice</h2>
<ul>
  <li>Instead of trying to learn something abstractly, pick a specific project you’d like to work on and learn as you go. This will make it much more clearer how certain information can be applied. 
<!-- - You don’t need to understand everything to start applying machine learning in a useful way (in fact, you might discover that almost all practitioners have knowledge gaps). --></li>
</ul>

<h2 id="desk">Desk</h2>
<ul>
  <li>Water</li>
  <li>Candle</li>
  <li>Tea</li>
  <li>Lamp</li>
  <li>Pencil</li>
  <li>Notebook</li>
  <li>Laptop</li>
  <li>Glasses</li>
</ul>

<h2 id="educational-resources">Educational resources</h2>
<ul>
  <li><a href="https://ocw.mit.edu">MIT OpenCourseWare</a> is a web-based publication of virtually all MIT course content.</li>
  <li><a href="https://www.khanacademy.org">Khan Academy</a> provides free, online educational content in a wide range of subjects, including math, science, history, economics, and computer science.</li>
  <li><a href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=coursera&amp;ie=UTF-8&amp;oe=UTF-8">Coursera</a> partners with top universities and organizations from around the world to provide high-quality educational content to learners of all backgrounds.</li>
  <li><a href="https://www.wolfram.com/wolfram-u/">Wolfram U</a> offers free open interactive courses, learning events and other educational resources for professional and technical development.</li>
</ul>

<h2 id="hacks">Hacks</h2>
<ul>
  <li>Place no-snooze alarm clock out of arms reach. (This will push you to get out of bed.)
<!-- - Avoid late-night eating by brushing teeth after last meal.  --></li>
  <li>Put away phone while reading; if unfamiliar with certain words, keep on hand a <a href="https://www.amazon.com/Paperback-Oxford-English-Dictionary-Dictionaries/dp/0199640947/ref=sr_1_1?crid=179MX4LD71LPV&amp;keywords=oxford+dictionary&amp;qid=1681314435&amp;s=audible&amp;sprefix=oxford+dictio%2Caudible%2C188&amp;sr=1-1-catcorr">physical dictionary</a>.</li>
  <li>Use a fan to drown out surrounding noise. Alternatively, play <a href="https://open.spotify.com/track/5UBonaClAZVfzxJNn8nnhh?si=574507e8b3744f00">white noise</a> on loop.</li>
  <li>Don’t use social media (this includes YouTube and Reddit). See <a href="https://www.netflix.com/title/81254224">The Social Dilemma</a>.</li>
</ul>

<h2 id="leisure">Leisure</h2>
<ul>
  <li>Spend time with family and friends.</li>
  <li>Read.</li>
  <li>Listen to audiobooks. (Use a library card to <a href="https://help.libbyapp.com/en-us/6144.htm">get access to thousands of audiobooks</a>.)</li>
</ul>

<h2 id="podcasts">Podcasts</h2>
<ul>
  <li><a href="https://open.spotify.com/show/5rgumWEx4FsqIY8e1wJNAk?si=0597ac75f6b14d5d">Making Sense</a> with Sam Harris.
    <ul>
      <li><a href="https://steno.ai/making-sense-with-sam-harris-14">Episode transcripts</a></li>
    </ul>
  </li>
  <li><a href="https://open.spotify.com/show/0e9lFr3AdJByoBpM6tAbxD?si=74930b088d064971">Deep Questions</a> with Cal Newport.
    <ul>
      <li><a href="https://steno.ai/deep-questions-with-cal-newport">Episode transcripts</a></li>
    </ul>
  </li>
  <li><a href="https://open.spotify.com/show/0DAMKPOlWsIFDw5uKOopir?si=46646345af1c4eac">The Stephen Wolfram Podcast</a> with Stephen Wolfram.
    <ul>
      <li><a href="https://steno.ai/the-stephen-wolfram-podcast">Episode transcripts</a></li>
    </ul>
  </li>
</ul>

<h2 id="meditation">Meditation</h2>
<ul>
  <li>Sam Harris’s <a href="https://www.wakingup.com">Waking Up App</a>.</li>
</ul>

<h2 id="nightly-routine">Nightly Routine</h2>
<ul>
  <li>Fill out a time-blocking planner. (I use <a href="https://www.timeblockplanner.com">this</a> one.)</li>
  <li>Prepare tomorrow’s clothes.</li>
  <li>Wear a <a href="https://www.amazon.com/Contoured-Sleeping-Blindfold-Concave-Meditation/dp/B07KC5DWCC/ref=zg_bs_3764231_sccl_1/132-0947972-7044716?th=1">sleep mask</a>.</li>
</ul>

<h2 id="software">Software</h2>
<ul>
  <li><a href="https://www.wolframalpha.com">Wolfram Alpha</a> is an answer engine developed by Wolfram Research.</li>
  <li><a href="https://www.wolfram.com/mathematica/">Mathematica</a> is a computational software program that provides a comprehensive environment for performing mathematical computations, data analysis, and visualizations.</li>
</ul>]]></content><author><name></name></author><category term="Productivity" /><summary type="html"><![CDATA[This post was inspired by Stephen Wolfram’s 2019 post.]]></summary></entry></feed>